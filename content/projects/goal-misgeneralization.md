---
title: Goal Misgeneralization
tags:
  - ai
  - ai-safety
  - reinforcement-learning
---

I co-led this research project at the [AI Safety Camp](https://aisafety.camp), producing the first empirical demonstrations of goal misgeneralization in deep reinforcement learning.

Goal misgeneralization is a failure mode where agents retain their capabilities but pursue the wrong objectives. For example, an agent might continue to competently avoid obstacles while navigating to the wrong destination. This is more insidious than typical failures where agents simply break down under distribution shift.

The paper was published at ICML 2022.

## Publications & Writing

1. [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111) (arXiv)
2. [Empirical Observations of Objective Robustness Failures](https://www.lesswrong.com/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures) (LessWrong)
3. [Discussion: Objective Robustness and Inner Alignment](https://www.lesswrong.com/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment) (LessWrong)
